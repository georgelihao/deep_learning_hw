{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6HNDhyqLjiP"
   },
   "source": [
    "# Important: You have to run logistic_regression_multi_5Layer.ipynb first before running this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:58:49.569152Z",
     "start_time": "2018-09-20T01:58:48.721454Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fXgZ9kSsLjib",
    "outputId": "7f3e7ac8-ca97-4aa6-d5c5-bf459c4e2bc8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# generate original training and test data\n",
    "img_size = 28\n",
    "n_classes = 10\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 784\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "\n",
      "Spliting data\n",
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "#option 1: load MNIST dataset \n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#option 2: load MNIST dataset \n",
    "print('\\nLoading MNIST')\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size])\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size])\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind]\n",
    "\n",
    "# 10% for validation \n",
    "validatationPct = 0.1\n",
    "n = int(x_train.shape[0] * (1-validatationPct))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "#\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]\n",
    "\n",
    "train_num_examples = x_train.shape[0]\n",
    "valid_num_examples = x_valid.shape[0]\n",
    "test_num_examples  = x_test.shape[0]\n",
    "\n",
    "print(train_num_examples, valid_num_examples, test_num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-19T22:03:55.082812Z",
     "start_time": "2018-09-19T22:03:55.079720Z"
    },
    "colab_type": "text",
    "id": "giq_0GNvLji4"
   },
   "source": [
    "# Parameters (have to be consistent with logistic_regression_multi_5layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:59:02.285613Z",
     "start_time": "2018-09-20T01:59:02.277606Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "nDGfBFFZLji9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Network Hidden Layers Parameters\n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 200\n",
    "n_hidden_3 = 200\n",
    "n_hidden_4 = 200\n",
    "n_hidden_5 = 200\n",
    "\n",
    "#Input and Output Layers Parameters\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "#change it to your own path \n",
    "#log_files_path = './checkpoints/'\n",
    "#log_files_path = 'C:/Users/ali.hirsa/logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "coxhf2tRLjjm"
   },
   "source": [
    "# Definition of the Layer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:59:03.814451Z",
     "start_time": "2018-09-20T01:59:03.796629Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "42P_qdYeLjjv"
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    print('inside layer')\n",
    "    \n",
    "    # comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0/weight_shape[0])**0.5\n",
    "    \n",
    "    #initialization of the weights\n",
    "    w_0 = tf.random_normal_initializer(stddev=w_std)\n",
    "\n",
    "    b_0 = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_0)\n",
    "    b = tf.get_variable(\"b\", bias_shape,   initializer=b_0)\n",
    "    \n",
    "    # different activation functions\n",
    "    # you can try either \n",
    "\n",
    "    #return tf.nn.relu(tf.matmul(x, W) + b)\n",
    "    return tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPdrT9HLLjkH"
   },
   "source": [
    "# Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:59:05.120581Z",
     "start_time": "2018-09-20T01:59:05.107111Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UoJkkp9CLjkP"
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    \"\"\"\n",
    "    define the whole network (2 hidden layers + output layers)\n",
    "    input:\n",
    "        - a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1])\n",
    "        #print([input_size, n_hidden_1])\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "        #print([n_hidden_1, n_hidden_2])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_3\"):\n",
    "        hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3])\n",
    "        #print([n_hidden_2, n_hidden_3])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_4\"):\n",
    "        hidden_4 = layer(hidden_3, [n_hidden_3, n_hidden_4], [n_hidden_4])\n",
    "        #print([n_hidden_3, n_hidden_4])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_layer_5\"):\n",
    "        hidden_5 = layer(hidden_4, [n_hidden_4, n_hidden_5], [n_hidden_5])\n",
    "        #print([n_hidden_4, n_hidden_5])\n",
    "     \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_5, [n_hidden_5, output_size], [output_size])\n",
    "        #print([n_hidden_5, output_size])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7zP-B-g5Ljkj"
   },
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:59:07.775648Z",
     "start_time": "2018-09-20T01:59:07.769327Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BQ2JWjT5Ljkk"
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    #Computes softmax cross entropy between logits and labels.\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cc8K5Bi4Ljkx"
   },
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-20T01:59:09.333152Z",
     "start_time": "2018-09-20T01:59:09.003257Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fBeDKFfhLjk8",
    "outputId": "f8c53377-0d48-4ac4-dc9d-8756bf92bdaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "WARNING:tensorflow:From <ipython-input-6-8bb28c0902cf>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./logs/multi_layer_5/model-checkpoint-98280\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "inside layer\n",
      "[40.11982, 5.16751]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "            \n",
    "#the input variables are first define as placeholder \n",
    "# a placeholder is a variable/data which will be assigned later \n",
    "# image vector & label\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "if not os.path.isdir('./logs/'):\n",
    "    os.makedirs('./logs/')\n",
    "log_files_path = './logs/'\n",
    "\n",
    "#defines a session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"multi_layer_5\", reuse=tf.AUTO_REUSE):\n",
    "    #opt neural network definition\n",
    "    \n",
    "    #the network is defined using the inference function defined above in the code\n",
    "    output_opt = inference(x)\n",
    "    \n",
    "    #compute the cost \n",
    "    cost_opt = loss(output_opt, y)\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    \n",
    "    var_list_opt = [\"hidden_layer_1/W\", \"hidden_layer_1/b\",\n",
    "                    \"hidden_layer_2/W\", \"hidden_layer_2/b\",\n",
    "                    \"hidden_layer_3/W\", \"hidden_layer_3/b\",\n",
    "                    \"hidden_layer_4/W\", \"hidden_layer_4/b\", \n",
    "                    \"hidden_layer_5/W\", \"hidden_layer_5/b\", \n",
    "                    \"output/W\", \"output/b\"]\n",
    "    \n",
    "    var_list_opt = [tf.get_variable(v) for v in var_list_opt]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    # restore values of parameters saved previously\n",
    "    # remember to download the model checkpoints files from canvas and save them in the corresponding path\n",
    "    \n",
    "    # important: your check point number may be different from this one\n",
    "    saver.restore(sess, log_files_path + 'multi_layer_5/model-checkpoint-98280')\n",
    "    #------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    \n",
    "with tf.variable_scope(\"multi_layer_init\", reuse=tf.AUTO_REUSE):\n",
    "    #rand neural network definition\n",
    "    \n",
    "    #the network is defined using the inference function defined above in the code\n",
    "    output_rand = inference(x)\n",
    "    #compute the rand cost\n",
    "    cost_rand = loss(output_rand, y)\n",
    "    \n",
    "    var_list_rand = [\"hidden_layer_1/W\", \"hidden_layer_1/b\", \n",
    "                     \"hidden_layer_2/W\", \"hidden_layer_2/b\",\n",
    "                     \"hidden_layer_3/W\", \"hidden_layer_3/b\", \n",
    "                     \"hidden_layer_4/W\", \"hidden_layer_4/b\", \n",
    "                     \"hidden_layer_5/W\", \"hidden_layer_5/b\", \n",
    "                     \"output/W\", \"output/b\"]\n",
    "    \n",
    "    var_list_rand = [tf.get_variable(v) for v in var_list_rand]\n",
    "    \n",
    "    #initialization of the variables\n",
    "    init_op = tf.variables_initializer(var_list_rand)\n",
    "    \n",
    "    sess.run(init_op)\n",
    "\n",
    "    \n",
    "\n",
    "#dictionary for the test dataset \n",
    "#used to evaluate accuracy \n",
    "feed_dict = {\n",
    "        x: x_test,\n",
    "        y: y_test,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#compute the loss for opt and rand networks\n",
    "print(sess.run([cost_opt, cost_rand], feed_dict=feed_dict))\n",
    "\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"multi_layer_inter\") as scope:\n",
    "    #initialize the placeholder alpha coefficient \n",
    "    #a placeholder is a variable/data which will be assigned later \n",
    "    alpha = tf.placeholder(\"float\", [1, 1])\n",
    "    \n",
    "    #the coefficients of the opt and rand networks are modulated with the alpha coefficient\n",
    "    #the coefficients of the opt and rand networks are modulated with the alpha coefficient\n",
    "    h1_W_inter = var_list_opt[0] * (1.0 - alpha) + var_list_rand[0] * (alpha)\n",
    "    h1_b_inter = var_list_opt[1] * (1.0 - alpha) + var_list_rand[1] * (alpha)\n",
    "    #\n",
    "    h2_W_inter = var_list_opt[2] * (1.0 - alpha) + var_list_rand[2] * (alpha)\n",
    "    h2_b_inter = var_list_opt[3] * (1.0 - alpha) + var_list_rand[3] * (alpha)\n",
    "    #\n",
    "    h3_W_inter = var_list_opt[4] * (1.0 - alpha) + var_list_rand[4] * (alpha)\n",
    "    h3_b_inter = var_list_opt[5] * (1.0 - alpha) + var_list_rand[5] * (alpha)\n",
    "    #\n",
    "    h4_W_inter = var_list_opt[6] * (1.0 - alpha) + var_list_rand[6] * (alpha)\n",
    "    h4_b_inter = var_list_opt[7] * (1.0 - alpha) + var_list_rand[7] * (alpha)\n",
    "    #\n",
    "    h5_W_inter = var_list_opt[8] * (1.0 - alpha) + var_list_rand[8] * (alpha)\n",
    "    h5_b_inter = var_list_opt[9] * (1.0 - alpha) + var_list_rand[9] * (alpha)\n",
    "    #\n",
    "    o_W_inter  = var_list_opt[10] * (1.0 - alpha) + var_list_rand[10] * (alpha)\n",
    "    o_b_inter  = var_list_opt[11] * (1.0 - alpha) + var_list_rand[11] * (alpha)\n",
    "    \n",
    "    h1_inter = tf.nn.relu(tf.matmul(x,        h1_W_inter) + h1_b_inter)\n",
    "    h2_inter = tf.nn.relu(tf.matmul(h1_inter, h2_W_inter) + h2_b_inter)\n",
    "    h3_inter = tf.nn.relu(tf.matmul(h2_inter, h3_W_inter) + h3_b_inter)\n",
    "    h4_inter = tf.nn.relu(tf.matmul(h3_inter, h4_W_inter) + h4_b_inter)\n",
    "    h5_inter = tf.nn.relu(tf.matmul(h4_inter, h5_W_inter) + h5_b_inter)\n",
    "    o_inter  = tf.nn.relu(tf.matmul(h5_inter, o_W_inter ) + o_b_inter)\n",
    "    \n",
    "    cost_inter = loss(o_inter, y)\n",
    "    tf.summary.scalar(\"interpolated_cost\", cost_inter)\n",
    "       \n",
    "#save the parameters for plotting in tensorboard\n",
    "summary_writer = tf.summary.FileWriter(log_files_path + 'linear_interpolation_5/', sess.graph)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "#list to save the results of the following for loop\n",
    "results = []\n",
    "alphaMin = -2\n",
    "alphaMax = 2\n",
    "alphaStep = 0.01\n",
    "for a in np.arange(alphaMin, alphaMax, alphaStep):\n",
    "    #test data with different values of alpha coefficient\n",
    "    feed_dict = {\n",
    "            x: x_test,\n",
    "            y: y_test,\n",
    "            alpha: [[a]],\n",
    "            }\n",
    "    #compute the loss for the different value of alpha\n",
    "    [cost, summary_str] = sess.run([cost_inter, summary_op], feed_dict=feed_dict)\n",
    "    summary_writer.add_summary(summary_str, (a + alphaMax)/alphaStep)\n",
    "    results.append(cost)\n",
    "\n",
    "plt.plot(np.arange(alphaMin, alphaMax, alphaStep), results, 'ro')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ALgPeppALjlv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "example_interpolation_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
