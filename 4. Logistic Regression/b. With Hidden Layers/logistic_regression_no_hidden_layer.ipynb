{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "\n",
      "Spliting data\n"
     ]
    }
   ],
   "source": [
    "#generate original training and test data\n",
    "img_size = 28\n",
    "img_chan = 1\n",
    "n_classes = 10\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "print('\\nLoading MNIST')\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size])\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size])\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "print(x_test.shape)\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind]\n",
    "\n",
    "# 10% for validation \n",
    "validatationPct = 0.1\n",
    "n = int(x_train.shape[0] * (1-validatationPct))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "#\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]\n",
    "\n",
    "\n",
    "# (Global) Parameters\n",
    "learning_rate = 0.05\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    #takes a batch of pictures as input and returns a batch of corresponding probabilities of being in each class\n",
    "    #input shape = (batch_size*image_size)     output shape = (batch_size*number_of_classes)\n",
    "    \n",
    "    init = tf.constant_initializer(value=0)\n",
    "\n",
    "    W = tf.get_variable(\"Weight\", [784, 10], initializer=init)\n",
    "    b = tf.get_variable(\"bias\", [10], initializer=init)\n",
    "\n",
    "    #This function performs the equivalent of softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "    #which returns a tensor with the same size as logits, the shape is batch_size*10 in this case \n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    # output and y have the same shape: batch_size * num_of_classes while the returned loss is a scaler tensor\n",
    "    # compute the average error per data sample by computing the cross-entropy loss over a minibatch\n",
    "    \n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    \n",
    "    #cross-entropy loss is more commonly used \n",
    "    #since the confidence of classification is taken into account\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    #tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.\n",
    "    xentropy = -tf.reduce_sum(dot_product, 1)\n",
    "    \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    # learning_rate \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Global_step refers to the number of batches seen of far. \n",
    "    # When it is passed in the minimize() argument list, the variable is increased by one.\n",
    "    # You can get the global_step value using tf.train.global_step()\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    # correct_prediction is a vector of boolean elements\n",
    "    # where \n",
    "    # true denotes prediction equals to the real value \n",
    "    # and \n",
    "    # false means the opposite\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    #tf.cast transfer boolean tensor into float tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 cost function= 0.6629273  Validation Error: 0.11516666412353516\n",
      "Epoch: 002 cost function= 0.4086371  Validation Error: 0.10366666316986084\n",
      "Epoch: 003 cost function= 0.3672313  Validation Error: 0.0976666808128357\n",
      "Epoch: 004 cost function= 0.3460498  Validation Error: 0.09500002861022949\n",
      "Epoch: 005 cost function= 0.3324737  Validation Error: 0.09266668558120728\n",
      "Epoch: 006 cost function= 0.3227667  Validation Error: 0.0910000205039978\n",
      "Epoch: 007 cost function= 0.3153570  Validation Error: 0.08866667747497559\n",
      "Epoch: 008 cost function= 0.3094482  Validation Error: 0.0871666669845581\n",
      "Epoch: 009 cost function= 0.3045859  Validation Error: 0.08583331108093262\n",
      "Epoch: 010 cost function= 0.3004885  Validation Error: 0.08533334732055664\n",
      "Epoch: 011 cost function= 0.2969707  Validation Error: 0.08516669273376465\n",
      "Epoch: 012 cost function= 0.2939047  Validation Error: 0.08433336019515991\n",
      "Epoch: 013 cost function= 0.2911992  Validation Error: 0.08366668224334717\n",
      "Epoch: 014 cost function= 0.2887867  Validation Error: 0.08366668224334717\n",
      "Epoch: 015 cost function= 0.2866163  Validation Error: 0.08283334970474243\n",
      "Epoch: 016 cost function= 0.2846488  Validation Error: 0.08233332633972168\n",
      "Epoch: 017 cost function= 0.2828533  Validation Error: 0.08249998092651367\n",
      "Epoch: 018 cost function= 0.2812052  Validation Error: 0.08266669511795044\n",
      "Epoch: 019 cost function= 0.2796845  Validation Error: 0.08133333921432495\n",
      "Epoch: 020 cost function= 0.2782751  Validation Error: 0.08016663789749146\n",
      "Epoch: 021 cost function= 0.2769632  Validation Error: 0.07983332872390747\n",
      "Epoch: 022 cost function= 0.2757378  Validation Error: 0.08016663789749146\n",
      "Epoch: 023 cost function= 0.2745892  Validation Error: 0.08016663789749146\n",
      "Epoch: 024 cost function= 0.2735093  Validation Error: 0.08033335208892822\n",
      "Epoch: 025 cost function= 0.2724913  Validation Error: 0.07999998331069946\n",
      "Epoch: 026 cost function= 0.2715290  Validation Error: 0.07966667413711548\n",
      "Epoch: 027 cost function= 0.2706174  Validation Error: 0.07966667413711548\n",
      "Epoch: 028 cost function= 0.2697519  Validation Error: 0.07966667413711548\n",
      "Epoch: 029 cost function= 0.2689285  Validation Error: 0.07966667413711548\n",
      "Epoch: 030 cost function= 0.2681437  Validation Error: 0.07916665077209473\n",
      "Epoch: 031 cost function= 0.2673944  Validation Error: 0.07883334159851074\n",
      "Epoch: 032 cost function= 0.2666778  Validation Error: 0.078166663646698\n",
      "Epoch: 033 cost function= 0.2659915  Validation Error: 0.07849997282028198\n",
      "Epoch: 034 cost function= 0.2653333  Validation Error: 0.07883334159851074\n",
      "Epoch: 035 cost function= 0.2647011  Validation Error: 0.07883334159851074\n",
      "Epoch: 036 cost function= 0.2640932  Validation Error: 0.07849997282028198\n",
      "Epoch: 037 cost function= 0.2635079  Validation Error: 0.07866668701171875\n",
      "Epoch: 038 cost function= 0.2629439  Validation Error: 0.078000009059906\n",
      "Epoch: 039 cost function= 0.2623996  Validation Error: 0.07766664028167725\n",
      "Epoch: 040 cost function= 0.2618740  Validation Error: 0.07716667652130127\n",
      "Epoch: 041 cost function= 0.2613659  Validation Error: 0.07700002193450928\n",
      "Epoch: 042 cost function= 0.2608742  Validation Error: 0.07683330774307251\n",
      "Epoch: 043 cost function= 0.2603980  Validation Error: 0.07683330774307251\n",
      "Epoch: 044 cost function= 0.2599366  Validation Error: 0.07683330774307251\n",
      "Epoch: 045 cost function= 0.2594889  Validation Error: 0.07666665315628052\n",
      "Epoch: 046 cost function= 0.2590544  Validation Error: 0.07649999856948853\n",
      "Epoch: 047 cost function= 0.2586323  Validation Error: 0.07633334398269653\n",
      "Epoch: 048 cost function= 0.2582221  Validation Error: 0.07616668939590454\n",
      "Epoch: 049 cost function= 0.2578230  Validation Error: 0.07616668939590454\n",
      "Epoch: 050 cost function= 0.2574346  Validation Error: 0.07599997520446777\n",
      "Epoch: 051 cost function= 0.2570563  Validation Error: 0.07599997520446777\n",
      "Epoch: 052 cost function= 0.2566877  Validation Error: 0.07583332061767578\n",
      "Epoch: 053 cost function= 0.2563284  Validation Error: 0.07566666603088379\n",
      "Epoch: 054 cost function= 0.2559778  Validation Error: 0.0753333568572998\n",
      "Epoch: 055 cost function= 0.2556357  Validation Error: 0.07483333349227905\n",
      "Epoch: 056 cost function= 0.2553017  Validation Error: 0.07483333349227905\n",
      "Epoch: 057 cost function= 0.2549753  Validation Error: 0.07466667890548706\n",
      "Epoch: 058 cost function= 0.2546564  Validation Error: 0.07466667890548706\n",
      "Epoch: 059 cost function= 0.2543445  Validation Error: 0.07483333349227905\n",
      "Epoch: 060 cost function= 0.2540395  Validation Error: 0.07483333349227905\n",
      "Epoch: 061 cost function= 0.2537409  Validation Error: 0.07499998807907104\n",
      "Epoch: 062 cost function= 0.2534487  Validation Error: 0.07450002431869507\n",
      "Epoch: 063 cost function= 0.2531625  Validation Error: 0.0743333101272583\n",
      "Epoch: 064 cost function= 0.2528821  Validation Error: 0.0743333101272583\n",
      "Epoch: 065 cost function= 0.2526073  Validation Error: 0.0743333101272583\n",
      "Epoch: 066 cost function= 0.2523379  Validation Error: 0.07416665554046631\n",
      "Epoch: 067 cost function= 0.2520737  Validation Error: 0.07400000095367432\n",
      "Epoch: 068 cost function= 0.2518145  Validation Error: 0.07400000095367432\n",
      "Epoch: 069 cost function= 0.2515601  Validation Error: 0.07383334636688232\n",
      "Epoch: 070 cost function= 0.2513105  Validation Error: 0.07366669178009033\n",
      "Epoch: 071 cost function= 0.2510653  Validation Error: 0.07366669178009033\n",
      "Epoch: 072 cost function= 0.2508245  Validation Error: 0.07366669178009033\n",
      "Epoch: 073 cost function= 0.2505880  Validation Error: 0.07383334636688232\n",
      "Epoch: 074 cost function= 0.2503555  Validation Error: 0.07383334636688232\n",
      "Epoch: 075 cost function= 0.2501270  Validation Error: 0.07383334636688232\n",
      "Epoch: 076 cost function= 0.2499023  Validation Error: 0.07400000095367432\n",
      "Epoch: 077 cost function= 0.2496813  Validation Error: 0.07400000095367432\n",
      "Epoch: 078 cost function= 0.2494640  Validation Error: 0.07400000095367432\n",
      "Epoch: 079 cost function= 0.2492501  Validation Error: 0.07383334636688232\n",
      "Epoch: 080 cost function= 0.2490397  Validation Error: 0.07400000095367432\n",
      "Epoch: 081 cost function= 0.2488325  Validation Error: 0.07400000095367432\n",
      "Epoch: 082 cost function= 0.2486285  Validation Error: 0.07400000095367432\n",
      "Epoch: 083 cost function= 0.2484277  Validation Error: 0.07416665554046631\n",
      "Epoch: 084 cost function= 0.2482298  Validation Error: 0.07400000095367432\n",
      "Epoch: 085 cost function= 0.2480349  Validation Error: 0.07400000095367432\n",
      "Epoch: 086 cost function= 0.2478429  Validation Error: 0.07416665554046631\n",
      "Epoch: 087 cost function= 0.2476536  Validation Error: 0.07400000095367432\n",
      "Epoch: 088 cost function= 0.2474670  Validation Error: 0.07416665554046631\n",
      "Epoch: 089 cost function= 0.2472831  Validation Error: 0.07416665554046631\n",
      "Epoch: 090 cost function= 0.2471018  Validation Error: 0.07416665554046631\n",
      "Epoch: 091 cost function= 0.2469229  Validation Error: 0.07416665554046631\n",
      "Epoch: 092 cost function= 0.2467465  Validation Error: 0.0743333101272583\n",
      "Epoch: 093 cost function= 0.2465724  Validation Error: 0.0743333101272583\n",
      "Epoch: 094 cost function= 0.2464007  Validation Error: 0.07416665554046631\n",
      "Epoch: 095 cost function= 0.2462312  Validation Error: 0.07416665554046631\n",
      "Epoch: 096 cost function= 0.2460640  Validation Error: 0.07400000095367432\n",
      "Epoch: 097 cost function= 0.2458989  Validation Error: 0.07383334636688232\n",
      "Epoch: 098 cost function= 0.2457359  Validation Error: 0.07383334636688232\n",
      "Epoch: 099 cost function= 0.2455749  Validation Error: 0.07400000095367432\n",
      "Epoch: 100 cost function= 0.2454159  Validation Error: 0.07400000095367432\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9254\n",
      "Execution time was 33.103\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #change it with your own path\n",
    "    log_files_path = 'C:/Users/ali.hirsa/logs/no_layer_1/'\n",
    "    #log_files_path = 'C:/Users/Ali/Google Drive/CoursesColumbiaUniveristy/DeepLearning/codes/week3/logs/no_layer/'\n",
    "\n",
    "    # read \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Graph\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "            \n",
    "        # first build the structure of our neural network\n",
    "\n",
    "        # variables has to be set up as placeholder before importing data\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # MNIST data image of shape 28*28=784\n",
    "\n",
    "        # y is the label in one-hot-encoding format\n",
    "        y = tf.placeholder(\"float\", [None, 10])  # 0-9 digits recognition\n",
    "\n",
    "        #output is a matrix of probabilities\n",
    "        output = inference(x)\n",
    "\n",
    "        cost = loss(output, y)\n",
    "        # set the initial value of global_step as 0\n",
    "        # this will increase by 1 every time weights are updated\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        train_op = training(cost, global_step)\n",
    "        #train_op = training(cost, global_step=None)\n",
    "\n",
    "        eval_op = evaluate(output, y)\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "        saver = tf.train.Saver()\n",
    "        #define a session\n",
    "        sess = tf.Session()\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(log_files_path, sess.graph)\n",
    "\n",
    "        #all variables need to be initialized by sess.run(tf.global_variables_initializer())\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init_op)\n",
    "\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.0\n",
    "            n_sample = x_train.shape[0]\n",
    "            total_batch = int((n_sample+batch_size-1) / batch_size)\n",
    "            #print('total_batch ', total_batch)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                start = i * batch_size\n",
    "                end = min(n_sample, start + batch_size)\n",
    "                minibatch_x = x_train[start:end]\n",
    "                minibatch_y = y_train[start:end]\n",
    "                \n",
    "                # Fit training using batch data\n",
    "                # Weights are only updated when we run the optimizer\n",
    "                sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                \n",
    "                # Compute average loss\n",
    "                avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                \n",
    "                # Get the accuracy by running the eval_op with validation sets of data\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: x_valid, y: y_valid})\n",
    "\n",
    "                print(\"Epoch:\", '%03d' % (epoch+1), \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "\n",
    "                summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "\n",
    "                #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        # Check the final accuracy after training\n",
    "        accuracy = sess.run(eval_op, feed_dict={x: x_test, y: y_test})\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        print('Execution time was %0.3f' % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
