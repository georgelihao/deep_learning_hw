{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.051382Z",
     "start_time": "2018-09-17T22:06:26.487771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# generate original training and test data\n",
    "img_size = 28\n",
    "n_classes = 10\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 784\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "\n",
      "Spliting data\n",
      "54000 6000 10000\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------\n",
    "#option 1: load MNIST dataset \n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------\n",
    "#option 2: load MNIST dataset \n",
    "print('\\nLoading MNIST')\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size])\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size])\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind]\n",
    "\n",
    "# 10% for validation \n",
    "validatationPct = 0.1\n",
    "n = int(x_train.shape[0] * (1-validatationPct))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "#\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]\n",
    "\n",
    "train_num_examples = x_train.shape[0]\n",
    "valid_num_examples = x_valid.shape[0]\n",
    "test_num_examples  = x_test.shape[0]\n",
    "\n",
    "print(train_num_examples, valid_num_examples, test_num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.107135Z",
     "start_time": "2018-09-17T22:06:31.090083Z"
    }
   },
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "#--------------------------------\n",
    "# learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "#training_epochs = 1000\n",
    "#batch_size = 30\n",
    "\n",
    "training_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "display_step = 10\n",
    "\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Two hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 200\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 300\n",
    "\n",
    "#MNIST data image of shape 28*28=784\n",
    "input_size = 784\n",
    "\n",
    "# 0-9 digits recognition (labels)\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.158231Z",
     "start_time": "2018-09-17T22:06:31.126508Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    # comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0/weight_shape[0])**0.5\n",
    "    #print(weight_shape[0])\n",
    "    #w_std = 0.5;\n",
    "\n",
    "    #initialization of the weights\n",
    "    #you can try either\n",
    "    w_0 = tf.random_normal_initializer(stddev=w_std)\n",
    "    #w_0 = tf.random_uniform_initializer(minval=-1,maxval=1)\n",
    "\n",
    "    b_0 = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_0)\n",
    "    b = tf.get_variable(\"b\", bias_shape,   initializer=b_0)\n",
    "    \n",
    "    print('Weight Matrix:', W)\n",
    "    print('Bias Vector:', b)\n",
    "    \n",
    "    # different activation functions\n",
    "    # you can try\n",
    "    # (1) linear activation (not a good idea)\n",
    "    #return tf.matmul(x, W) + b\n",
    "    # (2) tanh activation\n",
    "    #return tf.nn.tanh(tf.matmul(x, W) + b)\n",
    "    # (3) sigmoid activation\n",
    "    #return tf.nn.sigmoid(tf.matmul(x, W) + b)\n",
    "    # (4) relu activation\n",
    "    return tf.nn.relu(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.193619Z",
     "start_time": "2018-09-17T22:06:31.176269Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    \"\"\"\n",
    "    define the whole network (2 hidden layers + output layers)\n",
    "    input:\n",
    "        - a batch of pictures \n",
    "        (input shape = (batch_size*image_size))\n",
    "    output:\n",
    "        - a batch vector corresponding to the logits predicted by the network\n",
    "        (output shape = (batch_size*output_size)) \n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1])\n",
    "        #print([input_size, n_hidden_1])\n",
    "     \n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "        #print([n_hidden_1, n_hidden_2])\n",
    "     \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [n_hidden_2, output_size], [output_size])\n",
    "        #print([n_hidden_2, output_size])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define First Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.251904Z",
     "start_time": "2018-09-17T22:06:31.235310Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_1(output, y):\n",
    "    \"\"\"\n",
    "    computes the average error per data sample \n",
    "    by computing the cross-entropy loss over a minibatch\n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    #tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.\n",
    "    xentropy = -tf.reduce_sum(dot_product, 1)\n",
    "    \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T20:17:03.371287Z",
     "start_time": "2018-09-17T20:17:03.367055Z"
    }
   },
   "source": [
    "## Define Second Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.284838Z",
     "start_time": "2018-09-17T22:06:31.276738Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_2(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    #Computes softmax cross entropy between logits and labels.\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.313660Z",
     "start_time": "2018-09-17T22:06:31.305501Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    # tf.train.GradientDescentOptimizer\n",
    "    # will use different optimization routines \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:06:31.556011Z",
     "start_time": "2018-09-17T22:06:31.541378Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-17T22:27:56.024948Z",
     "start_time": "2018-09-17T22:27:29.279712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_1/W:0' shape=(784, 200) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_1/b:0' shape=(200,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/hidden_layer_2/W:0' shape=(200, 300) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/hidden_layer_2/b:0' shape=(300,) dtype=float32_ref>\n",
      "Weight Matrix: <tf.Variable 'multi_layer/output/W:0' shape=(300, 10) dtype=float32_ref>\n",
      "Bias Vector: <tf.Variable 'multi_layer/output/b:0' shape=(10,) dtype=float32_ref>\n",
      "WARNING:tensorflow:From <ipython-input-7-cbc6c9ace953>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 000 cost function= 0.2879462  Validation Error: 0.06466668844223022\n",
      "Epoch: 010 cost function= 0.0120492  Validation Error: 0.025499999523162842\n",
      "Epoch: 020 cost function= 0.0037546  Validation Error: 0.02283334732055664\n",
      "Epoch: 030 cost function= 0.0020052  Validation Error: 0.022166669368743896\n",
      "Epoch: 040 cost function= 0.0013242  Validation Error: 0.021666646003723145\n",
      "Epoch: 050 cost function= 0.0009632  Validation Error: 0.021499991416931152\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\.conda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 060 cost function= 0.0007395  Validation Error: 0.021166682243347168\n",
      "Epoch: 070 cost function= 0.0005918  Validation Error: 0.021000027656555176\n",
      "Epoch: 080 cost function= 0.0004603  Validation Error: 0.02133333683013916\n",
      "Epoch: 090 cost function= 0.0003945  Validation Error: 0.02133333683013916\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.98\n",
      "Execution time (seconds) was 190.901\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not os.path.isdir('./logs/'):\n",
    "        os.makedirs('./logs/')\n",
    "    log_files_path = './logs/'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"multi_layer_2\"):\n",
    "            #neural network definition \n",
    "            \n",
    "            #the input variables are first define as placeholder \n",
    "            # a placeholder is a variable/data which will be assigned later \n",
    "            # image vector & label\n",
    "            x = tf.placeholder(\"float\", [None, input_size])   # MNIST data image of shape 28*28=784\n",
    "            y = tf.placeholder(\"float\", [None, output_size])  # 0-9 digits recognition\n",
    "\n",
    "            #the network is defined using the inference function defined above in the code\n",
    "            output = inference(x)\n",
    "\n",
    "            cost = loss_2(output, y)\n",
    "            \n",
    "            #initialize the value of the global_step variable \n",
    "            # recall: it is incremented by one each time the .minimise() is called\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            \n",
    "            train_op = training(cost, global_step)\n",
    "            #train_op = training(cost, global_step=None)\n",
    "            \n",
    "            #evaluate the accuracy of the network (done on a validation set)\n",
    "            eval_op = evaluate(output, y)\n",
    "\n",
    "            summary_op = tf.summary.merge_all()\n",
    "    \n",
    "            #save and restore variables to and from checkpoints.\n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "            #defines a session\n",
    "            sess = tf.Session()\n",
    "            \n",
    "            # summary writer\n",
    "            #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "            #\n",
    "            summary_writer = tf.summary.FileWriter(log_files_path+'multi_layer_2/', sess.graph)\n",
    "        \n",
    "            #initialization of all the variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "        \n",
    "            #will work with this later\n",
    "            #saver.restore(sess, log_files_path+'multi_layer/model-checkpoint-66000')\n",
    "            \n",
    "            loss_trace = []\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "\n",
    "                avg_cost = 0.\n",
    "                \n",
    "                #total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                total_batch = int((train_num_examples+batch_size-1) / batch_size)\n",
    "            \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "\n",
    "                    #option 1\n",
    "                    #minibatch_x, minibatch_y = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "                    \n",
    "                    #option 2\n",
    "                    start = i * batch_size\n",
    "                    end = min(train_num_examples, start + batch_size)\n",
    "                    minibatch_x = x_train[start:end]\n",
    "                    minibatch_y = y_train[start:end]\n",
    "                    \n",
    "                    # Fit training using batch data\n",
    "                    #the training is done using the training dataset\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    \n",
    "                    # Compute average loss\n",
    "                    avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "                    \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    \n",
    "                    #option 1\n",
    "                    #accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "                    \n",
    "                    #option 2\n",
    "                    accuracy = sess.run(eval_op, feed_dict={x: x_valid, y: y_valid})\n",
    "                    \n",
    "                    loss_trace.append(1-accuracy)    \n",
    "                    print(\"Epoch:\", '%03d' % epoch, \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "                    summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                        \n",
    "                    #save to use later\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                    #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                    saver.save(sess, log_files_path + 'multi_layer/model-checkpoint', global_step=global_step)\n",
    "                        \n",
    "            print(\"Optimization Finished!\")\n",
    "            \n",
    "            #accuracy evaluated with the whole test dataset\n",
    "            \n",
    "            #option 1\n",
    "            #accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "            \n",
    "            #option 2\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: x_test, y: y_test})\n",
    "            print(\"Test Accuracy:\", accuracy)\n",
    "                    \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Execution time (seconds) was %0.3f' % elapsed_time)\n",
    "            \n",
    "            # Visualization of the results\n",
    "            # loss function\n",
    "            #plt.plot(loss_trace)\n",
    "            #plt.title('Cross Entropy Loss')\n",
    "            #plt.xlabel('epoch')\n",
    "            #plt.ylabel('loss')\n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
