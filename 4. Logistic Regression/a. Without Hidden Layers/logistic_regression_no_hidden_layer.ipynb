{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading MNIST\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "\n",
      "Spliting data\n"
     ]
    }
   ],
   "source": [
    "#load data. labels are in one-hot-encoding format\n",
    "#generate original training and test data\n",
    "img_size = 28\n",
    "n_classes = 10\n",
    "\n",
    "#global_step = \n",
    "input_size = 784\n",
    "output_size = 10\n",
    "\n",
    "print('\\nLoading MNIST')\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size])\n",
    "x_train = x_train.astype(np.float32)/255\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size])\n",
    "x_test = x_test.astype(np.float32)/255\n",
    "\n",
    "print(x_test.shape)\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test  = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind]\n",
    "\n",
    "# 10% for validation \n",
    "validatationPct = 0.1\n",
    "n = int(x_train.shape[0] * (1-validatationPct))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "#\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 270)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Global) Parameters\n",
    "learning_rate = 1.0\n",
    "training_epochs = 100\n",
    "batch_size = 200\n",
    "display_step = 1\n",
    "\n",
    "n_sample = x_train.shape[0]\n",
    "total_batch = int(x_train.shape[0]/batch_size)\n",
    "\n",
    "n_sample, total_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    #takes a batch of pictures as input and returns a batch of corresponding probabilities of being in each class\n",
    "    #input shape = (batch_size*image_size)     output shape = (batch_size*number_of_classes)\n",
    "    \n",
    "    init = tf.constant_initializer(value=0)\n",
    "\n",
    "    W = tf.get_variable(\"W\", [784, 10], initializer=init)\n",
    "    b = tf.get_variable(\"b\", [10], initializer=init)\n",
    "\n",
    "    #This function performs the equivalent of softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "    #which returns a tensor with the same size as logits, the shape is batch_size*10 in this case \n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    # output and y have the same shape: batch_size * num_of_classes while the returned loss is a scaler tensor\n",
    "    # compute the average error per data sample by computing the cross-entropy loss over a minibatch\n",
    "    \n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    \n",
    "    #cross-entropy loss is more commonly used since the confidence of classification is taken into account\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    #tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)\n",
    "    \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the optimizer and training target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "\n",
    "    #tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    # learning_rate \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Global_step refers to the number of batches seen of far. \n",
    "    # When it is passed in the minimize() argument list, the variable is increased by one.\n",
    "    # You can get the global_step value using tf.train.global_step()\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    #correct_prediction is a vector of boolean elements\n",
    "    #where true denotes prediction equals to the real value and false means the opposite\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    #tf.cast transfer boolean tensor into float tensor\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 cost function= 0.3828606  Validation Error: 0.093666672706604\n",
      "Epoch: 002 cost function= 0.2603395  Validation Error: 0.08950001001358032\n",
      "Epoch: 003 cost function= 0.2469993  Validation Error: 0.08749997615814209\n",
      "Epoch: 004 cost function= 0.2398309  Validation Error: 0.08583331108093262\n",
      "Epoch: 005 cost function= 0.2350717  Validation Error: 0.08499997854232788\n",
      "Epoch: 006 cost function= 0.2315630  Validation Error: 0.08516669273376465\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\.conda\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1058: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 007 cost function= 0.2288075  Validation Error: 0.08499997854232788\n",
      "Epoch: 008 cost function= 0.2265505  Validation Error: 0.08366668224334717\n",
      "Epoch: 009 cost function= 0.2246457  Validation Error: 0.08350002765655518\n",
      "Epoch: 010 cost function= 0.2230024  Validation Error: 0.08350002765655518\n",
      "Epoch: 011 cost function= 0.2215602  Validation Error: 0.08383333683013916\n",
      "Epoch: 012 cost function= 0.2202776  Validation Error: 0.08416664600372314\n",
      "Epoch: 013 cost function= 0.2191246  Validation Error: 0.08399999141693115\n",
      "Epoch: 014 cost function= 0.2180788  Validation Error: 0.08416664600372314\n",
      "Epoch: 015 cost function= 0.2171233  Validation Error: 0.08416664600372314\n",
      "Epoch: 016 cost function= 0.2162446  Validation Error: 0.08399999141693115\n",
      "Epoch: 017 cost function= 0.2154323  Validation Error: 0.08383333683013916\n",
      "Epoch: 018 cost function= 0.2146778  Validation Error: 0.08350002765655518\n",
      "Epoch: 019 cost function= 0.2139741  Validation Error: 0.08350002765655518\n",
      "Epoch: 020 cost function= 0.2133152  Validation Error: 0.08350002765655518\n",
      "Epoch: 021 cost function= 0.2126965  Validation Error: 0.08366668224334717\n",
      "Epoch: 022 cost function= 0.2121136  Validation Error: 0.08383333683013916\n",
      "Epoch: 023 cost function= 0.2115631  Validation Error: 0.08399999141693115\n",
      "Epoch: 024 cost function= 0.2110418  Validation Error: 0.08383333683013916\n",
      "Epoch: 025 cost function= 0.2105471  Validation Error: 0.08350002765655518\n",
      "Epoch: 026 cost function= 0.2100768  Validation Error: 0.08350002765655518\n",
      "Epoch: 027 cost function= 0.2096287  Validation Error: 0.08366668224334717\n",
      "Epoch: 028 cost function= 0.2092010  Validation Error: 0.08366668224334717\n",
      "Epoch: 029 cost function= 0.2087922  Validation Error: 0.08300000429153442\n",
      "Epoch: 030 cost function= 0.2084008  Validation Error: 0.08333331346511841\n",
      "Epoch: 031 cost function= 0.2080255  Validation Error: 0.08383333683013916\n",
      "Epoch: 032 cost function= 0.2076652  Validation Error: 0.08383333683013916\n",
      "Epoch: 033 cost function= 0.2073189  Validation Error: 0.08416664600372314\n",
      "Epoch: 034 cost function= 0.2069856  Validation Error: 0.08399999141693115\n",
      "Epoch: 035 cost function= 0.2066645  Validation Error: 0.08399999141693115\n",
      "Epoch: 036 cost function= 0.2063548  Validation Error: 0.08366668224334717\n",
      "Epoch: 037 cost function= 0.2060558  Validation Error: 0.08383333683013916\n",
      "Epoch: 038 cost function= 0.2057668  Validation Error: 0.08350002765655518\n",
      "Epoch: 039 cost function= 0.2054873  Validation Error: 0.08350002765655518\n",
      "Epoch: 040 cost function= 0.2052167  Validation Error: 0.08350002765655518\n",
      "Epoch: 041 cost function= 0.2049546  Validation Error: 0.08366668224334717\n",
      "Epoch: 042 cost function= 0.2047005  Validation Error: 0.08366668224334717\n",
      "Epoch: 043 cost function= 0.2044538  Validation Error: 0.08350002765655518\n",
      "Epoch: 044 cost function= 0.2042144  Validation Error: 0.08366668224334717\n",
      "Epoch: 045 cost function= 0.2039817  Validation Error: 0.08383333683013916\n",
      "Epoch: 046 cost function= 0.2037555  Validation Error: 0.08366668224334717\n",
      "Epoch: 047 cost function= 0.2035354  Validation Error: 0.08366668224334717\n",
      "Epoch: 048 cost function= 0.2033212  Validation Error: 0.08383333683013916\n",
      "Epoch: 049 cost function= 0.2031125  Validation Error: 0.08383333683013916\n",
      "Epoch: 050 cost function= 0.2029091  Validation Error: 0.08383333683013916\n",
      "Epoch: 051 cost function= 0.2027109  Validation Error: 0.08399999141693115\n",
      "Epoch: 052 cost function= 0.2025175  Validation Error: 0.08366668224334717\n",
      "Epoch: 053 cost function= 0.2023287  Validation Error: 0.08366668224334717\n",
      "Epoch: 054 cost function= 0.2021444  Validation Error: 0.08366668224334717\n",
      "Epoch: 055 cost function= 0.2019643  Validation Error: 0.08366668224334717\n",
      "Epoch: 056 cost function= 0.2017883  Validation Error: 0.08383333683013916\n",
      "Epoch: 057 cost function= 0.2016163  Validation Error: 0.08383333683013916\n",
      "Epoch: 058 cost function= 0.2014480  Validation Error: 0.08399999141693115\n",
      "Epoch: 059 cost function= 0.2012833  Validation Error: 0.08399999141693115\n",
      "Epoch: 060 cost function= 0.2011221  Validation Error: 0.08399999141693115\n",
      "Epoch: 061 cost function= 0.2009643  Validation Error: 0.08399999141693115\n",
      "Epoch: 062 cost function= 0.2008097  Validation Error: 0.08399999141693115\n",
      "Epoch: 063 cost function= 0.2006582  Validation Error: 0.08383333683013916\n",
      "Epoch: 064 cost function= 0.2005097  Validation Error: 0.08383333683013916\n",
      "Epoch: 065 cost function= 0.2003642  Validation Error: 0.08366668224334717\n",
      "Epoch: 066 cost function= 0.2002214  Validation Error: 0.08366668224334717\n",
      "Epoch: 067 cost function= 0.2000813  Validation Error: 0.08366668224334717\n",
      "Epoch: 068 cost function= 0.1999438  Validation Error: 0.08366668224334717\n",
      "Epoch: 069 cost function= 0.1998088  Validation Error: 0.08366668224334717\n",
      "Epoch: 070 cost function= 0.1996763  Validation Error: 0.08366668224334717\n",
      "Epoch: 071 cost function= 0.1995462  Validation Error: 0.08416664600372314\n",
      "Epoch: 072 cost function= 0.1994183  Validation Error: 0.08416664600372314\n",
      "Epoch: 073 cost function= 0.1992927  Validation Error: 0.0845000147819519\n",
      "Epoch: 074 cost function= 0.1991692  Validation Error: 0.0845000147819519\n",
      "Epoch: 075 cost function= 0.1990477  Validation Error: 0.08399999141693115\n",
      "Epoch: 076 cost function= 0.1989283  Validation Error: 0.08433336019515991\n",
      "Epoch: 077 cost function= 0.1988109  Validation Error: 0.0845000147819519\n",
      "Epoch: 078 cost function= 0.1986954  Validation Error: 0.0845000147819519\n",
      "Epoch: 079 cost function= 0.1985817  Validation Error: 0.0846666693687439\n",
      "Epoch: 080 cost function= 0.1984698  Validation Error: 0.0846666693687439\n",
      "Epoch: 081 cost function= 0.1983597  Validation Error: 0.0846666693687439\n",
      "Epoch: 082 cost function= 0.1982512  Validation Error: 0.0846666693687439\n",
      "Epoch: 083 cost function= 0.1981444  Validation Error: 0.08483332395553589\n",
      "Epoch: 084 cost function= 0.1980392  Validation Error: 0.0846666693687439\n",
      "Epoch: 085 cost function= 0.1979356  Validation Error: 0.0846666693687439\n",
      "Epoch: 086 cost function= 0.1978335  Validation Error: 0.0846666693687439\n",
      "Epoch: 087 cost function= 0.1977329  Validation Error: 0.08416664600372314\n",
      "Epoch: 088 cost function= 0.1976337  Validation Error: 0.08416664600372314\n",
      "Epoch: 089 cost function= 0.1975360  Validation Error: 0.08433336019515991\n",
      "Epoch: 090 cost function= 0.1974396  Validation Error: 0.08416664600372314\n",
      "Epoch: 091 cost function= 0.1973445  Validation Error: 0.08433336019515991\n",
      "Epoch: 092 cost function= 0.1972508  Validation Error: 0.08433336019515991\n",
      "Epoch: 093 cost function= 0.1971583  Validation Error: 0.08433336019515991\n",
      "Epoch: 094 cost function= 0.1970671  Validation Error: 0.08433336019515991\n",
      "Epoch: 095 cost function= 0.1969771  Validation Error: 0.08433336019515991\n",
      "Epoch: 096 cost function= 0.1968883  Validation Error: 0.08433336019515991\n",
      "Epoch: 097 cost function= 0.1968006  Validation Error: 0.0845000147819519\n",
      "Epoch: 098 cost function= 0.1967140  Validation Error: 0.0846666693687439\n",
      "Epoch: 099 cost function= 0.1966286  Validation Error: 0.08483332395553589\n",
      "Epoch: 100 cost function= 0.1965443  Validation Error: 0.08516669273376465\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9232\n",
      "Execution time was 69.282\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #change it with your own path\n",
    "    if not os.path.isdir('./logs/'):\n",
    "        os.makedirs('./logs/')\n",
    "    log_files_path = './logs/'\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        # first build the structure of our neural network\n",
    "        \n",
    "        # variables has to be set up as placeholder before importing data\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "        \n",
    "        # y is the label in one-hot-encoding format\n",
    "        y = tf.placeholder(\"float\", [None, 10])  # 0-9 digits recognition\n",
    "        \n",
    "        #output is a matrix of probabilities\n",
    "        output = inference(x)\n",
    "\n",
    "        cost = loss(output, y)\n",
    "        \n",
    "        # set the initial value of global_step as 0, this will increase by 1 every time weights are updated\n",
    "        global_step = tf.Variable(1, name='global_step', trainable=False)\n",
    "        train_op = training(cost, global_step)\n",
    "        \n",
    "        #train_op = training(cost, global_step=None)\n",
    "\n",
    "        eval_op = evaluate(output, y)\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "        saver = tf.train.Saver()\n",
    "        #define a session\n",
    "        sess = tf.Session()\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(log_files_path, sess.graph)\n",
    "\n",
    "        #all variables need to be initialized by sess.run(tf.global_variables_initializer())\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init_op)\n",
    "        \n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "\n",
    "            avg_cost = 0.0\n",
    "            \n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                \n",
    "                #print(' batch {0}/{1}'.format(batch + 1, n_batch))\n",
    "                start = i * batch_size\n",
    "                end = min(n_sample, start + batch_size)\n",
    "                minibatch_x, minibatch_y = x_train[start:end], y_train[start:end]\n",
    "                \n",
    "                # Fit training using batch data\n",
    "                # Weights are only updated when we run the optimizer\n",
    "                sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                \n",
    "                # Compute average loss\n",
    "                avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "                \n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                # Get the accuracy by running the eval_op with validation sets of data\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: x_valid, y: y_valid})\n",
    "\n",
    "                print(\"Epoch:\", '%03d' % (epoch+1), \"cost function=\", \"{:.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "\n",
    "                summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "                \n",
    "                #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "        # Check the final accuracy after training\n",
    "        accuracy = sess.run(eval_op, feed_dict={x: x_test, y: y_test})\n",
    "        print(\"Test Accuracy:\", accuracy)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print('Execution time was %.3f' % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
