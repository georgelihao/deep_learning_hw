{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dfd891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b613ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# CIFAR-10 image dimensions\n",
    "img_size = 32\n",
    "\n",
    "# CIFAR-10 images are 32x32 pixels with 3 color channels (RGB)\n",
    "input_size = 32 * 32 * 3\n",
    "\n",
    "# CIFAR-10 has 10 different classes\n",
    "n_classes = 10\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88a2a67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR-10\n",
      "\n",
      "Splitting data\n",
      "Training examples: 45000\n",
      "Validation examples: 5000\n",
      "Test examples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "print('\\nLoading CIFAR-10')\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Flatten the images for a fully connected network (if necessary)\n",
    "x_train = np.reshape(x_train, [-1, 32*32*3])\n",
    "x_test = np.reshape(x_test, [-1, 32*32*3])\n",
    "\n",
    "# Convert class vectors to binary class matrices (one-hot encoding)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Splitting data for validation\n",
    "print('\\nSplitting data')\n",
    "validation_pct = 0.1\n",
    "num_train = int(x_train.shape[0] * (1 - validation_pct))\n",
    "x_valid = x_train[num_train:]\n",
    "y_valid = y_train[num_train:]\n",
    "x_train = x_train[:num_train]\n",
    "y_train = y_train[:num_train]\n",
    "\n",
    "# Print the number of examples in each set\n",
    "train_num_examples = x_train.shape[0]\n",
    "valid_num_examples = x_valid.shape[0]\n",
    "test_num_examples = x_test.shape[0]\n",
    "\n",
    "print(f\"Training examples: {train_num_examples}\")\n",
    "print(f\"Validation examples: {valid_num_examples}\")\n",
    "print(f\"Test examples: {test_num_examples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f836ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "#--------------------------------\n",
    "# learning rate\n",
    "learning_rate = 0.05\n",
    "\n",
    "#training_epochs = 1000\n",
    "#batch_size = 30\n",
    "\n",
    "training_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d5807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, weight_shape, bias_shape, activation_function='relu'):\n",
    "    \"\"\"\n",
    "    Defines the network layers\n",
    "    input:\n",
    "        - x: input vector of the layer\n",
    "        - weight_shape: shape the the weight maxtrix\n",
    "        - bias_shape: shape of the bias vector\n",
    "    output:\n",
    "        - output vector of the layer after the matrix multiplication and non linear transformation\n",
    "    \"\"\"\n",
    "    \n",
    "    # comes from the study by He et al. for ReLU layers\n",
    "    w_std = (2.0/weight_shape[0])**0.5\n",
    "    #print(weight_shape[0])\n",
    "    #w_std = 0.5;\n",
    "\n",
    "    #initialization of the weights\n",
    "    #you can try either\n",
    "    w_0 = tf.random_normal_initializer(stddev=w_std)\n",
    "    #w_0 = tf.random_uniform_initializer(minval=-1,maxval=1)\n",
    "\n",
    "    b_0 = tf.constant_initializer(value=0)\n",
    "    \n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_0)\n",
    "    b = tf.get_variable(\"b\", bias_shape,   initializer=b_0)\n",
    "    \n",
    "    print('Weight Matrix:', W)\n",
    "    print('Bias Vector:', b)\n",
    "\n",
    "    # (1) linear activation (not a good idea)\n",
    "    linear_output = tf.matmul(x, W) + b\n",
    "    \n",
    "    # (2) tanh activation\n",
    "    if activation_function == 'tanh':\n",
    "        return tf.nn.tanh(linear_output)\n",
    "    # (3) sigmoid activation\n",
    "    elif activation_function == 'sigmoid':\n",
    "        return tf.nn.sigmoid(linear_output)\n",
    "    # (4) leaky_relu activation\n",
    "    elif activation_function == 'leaky_relu':\n",
    "        return tf.nn.leaky_relu(linear_output)\n",
    "    # (5) relu activation\n",
    "    elif activation_function == 'relu':\n",
    "        return tf.nn.relu(linear_output)\n",
    "    # (6) linear output\n",
    "    else:\n",
    "        return linear_output  # Linear activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c81972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_1(output, y):\n",
    "    \"\"\"\n",
    "    computes the average error per data sample \n",
    "    by computing the cross-entropy loss over a minibatch\n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "    dot_product = y * tf.log(output)\n",
    "    \n",
    "    #tf.reduce_sum: Computes the sum of elements across dimensions of a tensor.\n",
    "    xentropy = -tf.reduce_sum(dot_product, 1)\n",
    "    \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor.\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2c2a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_2(output, y):\n",
    "    \"\"\"\n",
    "    Computes softmax cross entropy between logits and labels and then the loss \n",
    "    \n",
    "    intput:\n",
    "        - output: the output of the inference function \n",
    "        - y: true value of the sample batch\n",
    "        \n",
    "        the two have the same shape (batch_size * num_of_classes)\n",
    "    output:\n",
    "        - loss: loss of the corresponding batch (scalar tensor)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #mean square error\n",
    "    #loss = tf.reduce_mean(tf.reduce_sum(tf.square(y-output)))\n",
    "    \n",
    "    #Computes softmax cross entropy between logits and labels.\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cb76aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    \"\"\"\n",
    "    defines the necessary elements to train the network\n",
    "    \n",
    "    intput:\n",
    "        - cost: the cost is the loss of the corresponding batch\n",
    "        - global_step: number of batch seen so far, it is incremented by one \n",
    "        each time the .minimize() function is called\n",
    "    \"\"\"\n",
    "\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    \n",
    "    # tf.train.GradientDescentOptimizer\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # try `tf.train.RMSPropOptimizer` as desired\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cea4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy on the validation set \n",
    "    input:\n",
    "        -output: prediction vector of the network for the validation set\n",
    "        -y: true value for the validation set\n",
    "    output:\n",
    "        - accuracy: accuracy on the validation set (scalar between 0 and 1)\n",
    "    \"\"\"\n",
    "    #correct prediction is a binary vector which equals one when the output and y match\n",
    "    #otherwise the vector equals 0\n",
    "    #tf.cast: change the type of a tensor into another one\n",
    "    #then, by taking the mean of the tensor, we directly have the average score, so the accuracy\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    tf.summary.scalar(\"validation_error\", (1.0 - accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebcc2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(layer_name):\n",
    "    if __name__ == '__main__':\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not os.path.isdir('./logs/'):\n",
    "            os.makedirs('./logs/')\n",
    "        log_files_path = './logs/'\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "\n",
    "            with tf.variable_scope(\"layer_name\"):\n",
    "                #neural network definition \n",
    "\n",
    "                #the input variables are first define as placeholder \n",
    "                # a placeholder is a variable/data which will be assigned later \n",
    "                # image vector & label\n",
    "                x = tf.placeholder(\"float\", [None, input_size])   # MNIST data image of shape 28*28=784\n",
    "                y = tf.placeholder(\"float\", [None, output_size])  # 0-9 digits recognition\n",
    "\n",
    "                #the network is defined using the inference function defined above in the code\n",
    "                output = inference(x)\n",
    "\n",
    "                cost = loss_2(output, y)\n",
    "\n",
    "                #initialize the value of the global_step variable \n",
    "                # recall: it is incremented by one each time the .minimise() is called\n",
    "                global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "                train_op = training(cost, global_step)\n",
    "                #train_op = training(cost, global_step=None)\n",
    "\n",
    "                #evaluate the accuracy of the network (done on a validation set)\n",
    "                eval_op = evaluate(output, y)\n",
    "\n",
    "                summary_op = tf.summary.merge_all()\n",
    "\n",
    "                #save and restore variables to and from checkpoints.\n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "                #defines a session\n",
    "                sess = tf.Session()\n",
    "\n",
    "                # summary writer\n",
    "                #https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter\n",
    "                #\n",
    "                summary_writer = tf.summary.FileWriter(log_files_path + '{layer}/'.format(layer = layer_name), sess.graph)\n",
    "\n",
    "                #initialization of all the variables\n",
    "                init_op = tf.global_variables_initializer()\n",
    "                sess.run(init_op)\n",
    "\n",
    "                #will work with this later\n",
    "                #saver.restore(sess, log_files_path+'multi_layer/model-checkpoint-66000')\n",
    "\n",
    "                loss_trace = []\n",
    "\n",
    "                # Training cycle\n",
    "                for epoch in range(training_epochs):\n",
    "\n",
    "                    avg_cost = 0.\n",
    "\n",
    "                    #total_batch = int(mnist.train.num_examples/batch_size)\n",
    "                    total_batch = int((train_num_examples+batch_size-1) / batch_size)\n",
    "\n",
    "                    # Loop over all batches\n",
    "                    for i in range(total_batch):\n",
    "\n",
    "                        #option 1\n",
    "                        #minibatch_x, minibatch_y = mnist.train.next_batch(batch_size, shuffle=False)\n",
    "\n",
    "                        #option 2\n",
    "                        start = i * batch_size\n",
    "                        end = min(train_num_examples, start + batch_size)\n",
    "                        minibatch_x = x_train[start:end]\n",
    "                        minibatch_y = y_train[start:end]\n",
    "\n",
    "                        # Fit training using batch data\n",
    "                        #the training is done using the training dataset\n",
    "                        sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "\n",
    "                        # Compute average loss\n",
    "                        avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "\n",
    "                    # Display logs per epoch step\n",
    "                    if epoch % display_step == 0:\n",
    "\n",
    "                        #the accuracy is evaluated using the validation dataset\n",
    "\n",
    "                        #option 1\n",
    "                        #accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "\n",
    "                        #option 2\n",
    "                        accuracy = sess.run(eval_op, feed_dict={x: x_valid, y: y_valid})\n",
    "                        loss_trace.append(1-accuracy)    \n",
    "                        print(\"Epoch:\", '%03d' % epoch, \"cost function=\", \"{:0.7f}\".format(avg_cost), \" Validation Error:\", (1.0 - accuracy))\n",
    "                        summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                        summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "\n",
    "                        #save to use later\n",
    "                        #https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "                        #saver.save(sess, log_files_path+'model-checkpoint', global_step=global_step)\n",
    "                        saver.save(sess, log_files_path+'{layer}/model-checkpoint'.format(layer = layer_name), global_step=global_step)\n",
    "\n",
    "                print(\"Optimization Finished!\")\n",
    "                #accuracy evaluated with the whole test dataset\n",
    "\n",
    "                #option 1\n",
    "                #accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "                #option 2\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: x_test, y: y_test})\n",
    "                print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print('Execution time (seconds) was %0.3f' % elapsed_time)\n",
    "\n",
    "                # Visualization of the results\n",
    "                # loss function\n",
    "                #plt.plot(loss_trace)\n",
    "                #plt.title('Cross Entropy Loss')\n",
    "                #plt.xlabel('epoch')\n",
    "                #plt.ylabel('loss')\n",
    "                #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbddeed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_1/W:0' shape=(3072, 200) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_1/b:0' shape=(200,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_2/W:0' shape=(200, 200) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_2/b:0' shape=(200,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_3/W:0' shape=(200, 200) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_3/b:0' shape=(200,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/output/W:0' shape=(200, 10) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/output/b:0' shape=(10,) dtype=float32>\n",
      "WARNING:tensorflow:From /Users/georgelihao/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /Users/georgelihao/anaconda3/lib/python3.11/site-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 20:23:09.607795: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 cost function= 6.7399019  Validation Error: 0.9024000018835068\n",
      "Epoch: 010 cost function= 10.1236504  Validation Error: 0.8962000012397766\n",
      "Epoch: 020 cost function= 10.6184374  Validation Error: 0.9050000011920929\n",
      "Epoch: 030 cost function= 9.9545677  Validation Error: 0.9024000018835068\n",
      "Epoch: 040 cost function= 9.8812203  Validation Error: 0.9024000018835068\n",
      "Epoch: 050 cost function= 10.6476730  Validation Error: 0.9024000018835068\n",
      "WARNING:tensorflow:From /Users/georgelihao/anaconda3/lib/python3.11/site-packages/tensorflow/python/training/saver.py:1067: remove_checkpoint (from tensorflow.python.checkpoint.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Epoch: 060 cost function= 10.5869318  Validation Error: 0.9013999998569489\n",
      "Epoch: 070 cost function= 11.1056826  Validation Error: 0.9024000018835068\n",
      "Epoch: 080 cost function= 11.1319421  Validation Error: 0.8941999971866608\n",
      "Epoch: 090 cost function= 11.1444225  Validation Error: 0.9024000018835068\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.1\n",
      "Execution time (seconds) was 159.389\n"
     ]
    }
   ],
   "source": [
    "# updated parameters - 200 neurons\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Three hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 200\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 200\n",
    "# number of neurons in layer 3\n",
    "n_hidden_3 = 200\n",
    "\n",
    "def inference(x):\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1], 'tanh')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], 'sigmoid')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_3\"):\n",
    "        hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], 'leaky_relu')\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_3, [n_hidden_3, output_size], [output_size], 'linear')  # Linear activation for output\n",
    "\n",
    "    return output\n",
    "\n",
    "main_function('multi_layer_4_1_200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2789595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_1/W:0' shape=(3072, 100) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_1/b:0' shape=(100,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_2/W:0' shape=(100, 100) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_2/b:0' shape=(100,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_3/W:0' shape=(100, 100) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_3/b:0' shape=(100,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_4/W:0' shape=(100, 100) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_4/b:0' shape=(100,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/output/W:0' shape=(100, 10) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/output/b:0' shape=(10,) dtype=float32>\n",
      "Epoch: 000 cost function= 2.2975094  Validation Error: 0.9013999998569489\n",
      "Epoch: 010 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 020 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 030 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 040 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 050 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 060 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 070 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 080 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 090 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.1\n",
      "Execution time (seconds) was 113.418\n"
     ]
    }
   ],
   "source": [
    "# updated parameters - 100 neurons\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Four hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 100\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 100\n",
    "# number of neurons in layer 3\n",
    "n_hidden_3 = 100\n",
    "# number of neurons in layer 4\n",
    "n_hidden_4 = 100\n",
    "\n",
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1], 'tanh')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], 'sigmoid')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_3\"):\n",
    "        hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], 'sigmoid')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_4\"):\n",
    "        hidden_4 = layer(hidden_3, [n_hidden_3, n_hidden_4], [n_hidden_4], 'relu')\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_4, [n_hidden_4, output_size], [output_size], 'linear')  # Linear activation for output\n",
    "\n",
    "    return output\n",
    "\n",
    "main_function('multi_layer_4_2_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1005432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_1/W:0' shape=(3072, 50) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_1/b:0' shape=(50,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_2/W:0' shape=(50, 50) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_2/b:0' shape=(50,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_3/W:0' shape=(50, 50) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_3/b:0' shape=(50,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/hidden_layer_4/W:0' shape=(50, 50) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/hidden_layer_4/b:0' shape=(50,) dtype=float32>\n",
      "Weight Matrix: <tf.Variable 'layer_name/output/W:0' shape=(50, 10) dtype=float32>\n",
      "Bias Vector: <tf.Variable 'layer_name/output/b:0' shape=(10,) dtype=float32>\n",
      "Epoch: 000 cost function= 2.2903074  Validation Error: 0.9013999998569489\n",
      "Epoch: 010 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 020 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 030 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 040 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 050 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 060 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 070 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 080 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Epoch: 090 cost function= 2.2887199  Validation Error: 0.9013999998569489\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.1\n",
      "Execution time (seconds) was 81.280\n"
     ]
    }
   ],
   "source": [
    "# updated parameters - 50 neurons\n",
    "#Network Architecture\n",
    "# -----------------------------------------\n",
    "#\n",
    "# Four hidden layers\n",
    "#\n",
    "#------------------------------------------\n",
    "# number of neurons in layer 1\n",
    "n_hidden_1 = 50\n",
    "# number of neurons in layer 2\n",
    "n_hidden_2 = 50\n",
    "# number of neurons in layer 3\n",
    "n_hidden_3 = 50\n",
    "# number of neurons in layer 4\n",
    "n_hidden_4 = 50\n",
    "\n",
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_layer_1\"):\n",
    "        hidden_1 = layer(x, [input_size, n_hidden_1], [n_hidden_1], 'tanh')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], 'sigmoid')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_3\"):\n",
    "        hidden_3 = layer(hidden_2, [n_hidden_2, n_hidden_3], [n_hidden_3], 'sigmoid')\n",
    "\n",
    "    with tf.variable_scope(\"hidden_layer_4\"):\n",
    "        hidden_4 = layer(hidden_3, [n_hidden_3, n_hidden_4], [n_hidden_4], 'relu')\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_4, [n_hidden_4, output_size], [output_size], 'linear')  # Linear activation for output\n",
    "\n",
    "    return output\n",
    "\n",
    "main_function('multi_layer_4_2_50')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
