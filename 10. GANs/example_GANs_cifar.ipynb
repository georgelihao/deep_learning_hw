{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:36:48.362174Z",
     "start_time": "2018-12-11T03:35:00.880932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CIFAR10\n",
      "\n",
      "Spliting data\n",
      "45000 5000 10000\n"
     ]
    }
   ],
   "source": [
    "# The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images \n",
    "# that are commonly used to train machine learning and computer vision algorithms\n",
    "\n",
    "# generate original training and test data\n",
    "# The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, \n",
    "# with 6000 images per class. \n",
    "# There are 50000 training images and 10000 test images.\n",
    "img_size = 32\n",
    "img_chan = 3\n",
    "n_classes = 10\n",
    "\n",
    "learning_rate = 0.05\n",
    "#global_step = \n",
    "input_size = 32*32*3\n",
    "output_size = 10\n",
    "\n",
    "\n",
    "print('\\nLoading CIFAR10')\n",
    "\n",
    "cifar10 = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#x_train = np.reshape(x_train, [-1, img_size, img_size, img_chan])\n",
    "x_train = np.reshape(x_train, [-1, img_size*img_size*3])\n",
    "x_train = x_train.astype(np.float32) / 255\n",
    "#x_test = np.reshape(X_test, [-1, img_size, img_size, img_chan])\n",
    "x_test = np.reshape(x_test, [-1, img_size*img_size*3])\n",
    "x_test = x_test.astype(np.float32) / 255\n",
    "\n",
    "to_categorical = tf.keras.utils.to_categorical \n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print('\\nSpliting data')\n",
    "\n",
    "ind = np.random.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[ind], y_train[ind] \n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "n = int(x_train.shape[0] * (1-VALIDATION_SPLIT))\n",
    "x_valid = x_train[n:]\n",
    "x_train = x_train[:n]\n",
    "y_valid = y_train[n:]\n",
    "y_train = y_train[:n] \n",
    "\n",
    "\n",
    "train_num_examples = x_train.shape[0]\n",
    "valid_num_examples = x_valid.shape[0]\n",
    "test_num_examples  = x_test.shape[0]\n",
    "\n",
    "print(train_num_examples, valid_num_examples, test_num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:57.817911Z",
     "start_time": "2018-12-11T03:43:57.736829Z"
    }
   },
   "outputs": [],
   "source": [
    "#CIFAR data image of shape 32*32*3=3072\n",
    "#CIFAR_size = 32*32*3\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3072])\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameters for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:58.386340Z",
     "start_time": "2018-12-11T03:43:58.160243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "# Define the variables for the generator, we will use them to build layers later\n",
    "# -------------------\n",
    "size_g_w1 = 100\n",
    "size_g_b1 = 512\n",
    "# A good way to decide the std for initializing the weights\n",
    "w1_std = 1.0/tf.sqrt(size_g_w1/2.0)\n",
    "\n",
    "G_W1 = tf.Variable(tf.random_normal(shape=[size_g_w1, size_g_b1], stddev=w1_std))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[size_g_b1]))\n",
    "\n",
    "size_g_w2 = 512\n",
    "size_g_b2 = 3072\n",
    "w2_std = 1.0/tf.sqrt(size_g_w2/2.0)\n",
    "\n",
    "G_W2 = tf.Variable(tf.random_normal(shape=[size_g_w2, size_g_b2], stddev=w2_std))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[size_g_b2]))\n",
    "# theta_G and theta_D will be feeded to different optimizers later as \"var_list\", \n",
    "# since currently we have two networks instead of one now.\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "# ====================\n",
    "# Discriminator\n",
    "# Define the variables for the discriminator\n",
    "# --------------------\n",
    "size_d_w1 = 3072\n",
    "size_d_b1 = 512\n",
    "w1_std = 1.0/tf.sqrt(size_d_w1/2.0)\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal(shape=[size_d_w1,size_d_b1], stddev=w1_std))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[size_d_b1]))\n",
    "\n",
    "size_d_w2 = 512\n",
    "size_d_b2 = 1\n",
    "w2_std = 1.0/tf.sqrt(size_d_w2/2.0)\n",
    "\n",
    "D_W2 = tf.Variable(tf.random_normal(shape=[size_d_w2,size_d_b2], stddev=w2_std))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[size_d_b2]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:43:58.893933Z",
     "start_time": "2018-12-11T03:43:58.883749Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_logit)\n",
    "\n",
    "    return G_prob, G_logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:00.006536Z",
     "start_time": "2018-12-11T03:43:59.998146Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate samples function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:01.081943Z",
     "start_time": "2018-12-11T03:44:01.078026Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    # randomly generate samples for generator\n",
    "    return np.random.uniform(-1.0, 1.0, size = [m, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:02.009856Z",
     "start_time": "2018-12-11T03:44:01.990399Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sample(samples, size1, size2):\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(size1, size2))\n",
    "    gs = gridspec.GridSpec(size1, size2)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        sample_reshaped=sample.reshape(32,32,3)\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample_reshaped[:,:,0], cmap='gray')\n",
    "        plt.imshow(sample_reshaped[:,:,1], cmap='gray')\n",
    "        plt.imshow(sample_reshaped[:,:,2], cmap='gray')\n",
    "\n",
    "    return fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faciliate the path defining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:07.984901Z",
     "start_time": "2018-12-11T03:44:07.977698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Though it's not possible to get the path to the notebook by __file__, os.path is still very useful in dealing with paths and files\n",
    "# In this case, we can use an alternative: pathlib.Path\n",
    "\"\"\"\n",
    "code_dir   = os.path.dirname(__file__)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "#get the current path of our code\n",
    "code_dir = Path().resolve()\n",
    "#create output_dir within the same path\n",
    "output_dir = os.path.join(code_dir, 'outputGANs/')\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build GNN with defined vars and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:11.381835Z",
     "start_time": "2018-12-11T03:44:10.673336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Put randomly generated sample Z into the generator to create \"fake\" images\n",
    "G_sample, _ = generator(Z)\n",
    "# The result of discriminator of real and fake samples\n",
    "_, D_logit_real = discriminator(X)\n",
    "_, D_logit_fake = discriminator(G_sample)\n",
    "\n",
    "# generator loss \n",
    "# the goal of generator is to let discriminator make more mistakes on fake samples\n",
    "# tf.ones_like returns a tensor with all elements set to 1\n",
    "# 0 represent fake and 1 means real\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "# discriminator loss \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:44:11.885713Z",
     "start_time": "2018-12-11T03:44:11.879432Z"
    }
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size = 5, from_test=False):\n",
    "    \n",
    "    if from_test:\n",
    "        return images_test,labels_test\n",
    "    else:\n",
    "        index = np.random.choice(list(range(len(images_train))),size=batch_size)\n",
    "        return images_train[index], labels_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T03:50:44.091890Z",
     "start_time": "2018-12-11T03:44:13.192614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "D_loss: 1.406\n",
      "G_loss: 5.815\n",
      "\n",
      "iteration: 1000\n",
      "D_loss: 0.4738\n",
      "G_loss: 3.623\n",
      "\n",
      "iteration: 2000\n",
      "D_loss: 1.434\n",
      "G_loss: 1.868\n",
      "\n",
      "iteration: 3000\n",
      "D_loss: 2.265\n",
      "G_loss: 1.374\n",
      "\n",
      "iteration: 4000\n",
      "D_loss: 1.296\n",
      "G_loss: 2.638\n",
      "\n",
      "iteration: 5000\n",
      "D_loss: 1.39\n",
      "G_loss: 0.87\n",
      "\n",
      "iteration: 6000\n",
      "D_loss: 2.237\n",
      "G_loss: 0.8842\n",
      "\n",
      "iteration: 7000\n",
      "D_loss: 1.39\n",
      "G_loss: 1.786\n",
      "\n",
      "iteration: 8000\n",
      "D_loss: 1.723\n",
      "G_loss: 0.6481\n",
      "\n",
      "iteration: 9000\n",
      "D_loss: 0.9782\n",
      "G_loss: 1.495\n",
      "\n",
      "iteration: 10000\n",
      "D_loss: 0.451\n",
      "G_loss: 3.18\n",
      "\n",
      "iteration: 11000\n",
      "D_loss: 1.433\n",
      "G_loss: 1.042\n",
      "\n",
      "iteration: 12000\n",
      "D_loss: 1.029\n",
      "G_loss: 1.076\n",
      "\n",
      "iteration: 13000\n",
      "D_loss: 1.214\n",
      "G_loss: 1.093\n",
      "\n",
      "iteration: 14000\n",
      "D_loss: 1.394\n",
      "G_loss: 0.7071\n",
      "\n",
      "iteration: 15000\n",
      "D_loss: 0.9233\n",
      "G_loss: 1.352\n",
      "\n",
      "iteration: 16000\n",
      "D_loss: 1.356\n",
      "G_loss: 0.7046\n",
      "\n",
      "iteration: 17000\n",
      "D_loss: 1.31\n",
      "G_loss: 0.676\n",
      "\n",
      "iteration: 18000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6953\n",
      "\n",
      "iteration: 19000\n",
      "D_loss: 1.44\n",
      "G_loss: 0.7798\n",
      "\n",
      "iteration: 20000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6892\n",
      "\n",
      "iteration: 21000\n",
      "D_loss: 1.389\n",
      "G_loss: 0.6932\n",
      "\n",
      "iteration: 22000\n",
      "D_loss: 1.383\n",
      "G_loss: 0.6554\n",
      "\n",
      "iteration: 23000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6933\n",
      "\n",
      "iteration: 24000\n",
      "D_loss: 1.388\n",
      "G_loss: 0.6516\n",
      "\n",
      "iteration: 25000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6899\n",
      "\n",
      "iteration: 26000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6931\n",
      "\n",
      "iteration: 27000\n",
      "D_loss: 1.385\n",
      "G_loss: 0.6928\n",
      "\n",
      "iteration: 28000\n",
      "D_loss: 1.389\n",
      "G_loss: 0.6875\n",
      "\n",
      "iteration: 29000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6931\n",
      "\n",
      "iteration: 30000\n",
      "D_loss: 1.384\n",
      "G_loss: 0.695\n",
      "\n",
      "iteration: 31000\n",
      "D_loss: 1.36\n",
      "G_loss: 0.9828\n",
      "\n",
      "iteration: 32000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.7232\n",
      "\n",
      "iteration: 33000\n",
      "D_loss: 1.385\n",
      "G_loss: 0.6934\n",
      "\n",
      "iteration: 34000\n",
      "D_loss: 1.387\n",
      "G_loss: 0.6981\n",
      "\n",
      "iteration: 35000\n",
      "D_loss: 1.382\n",
      "G_loss: 0.6634\n",
      "\n",
      "iteration: 36000\n",
      "D_loss: 1.385\n",
      "G_loss: 0.6947\n",
      "\n",
      "iteration: 37000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6835\n",
      "\n",
      "iteration: 38000\n",
      "D_loss: 1.381\n",
      "G_loss: 0.6952\n",
      "\n",
      "iteration: 39000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6932\n",
      "\n",
      "iteration: 40000\n",
      "D_loss: 1.213\n",
      "G_loss: 1.011\n",
      "\n",
      "iteration: 41000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.7\n",
      "\n",
      "iteration: 42000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6931\n",
      "\n",
      "iteration: 43000\n",
      "D_loss: 1.382\n",
      "G_loss: 0.6494\n",
      "\n",
      "iteration: 44000\n",
      "D_loss: 1.39\n",
      "G_loss: 0.7121\n",
      "\n",
      "iteration: 45000\n",
      "D_loss: 1.386\n",
      "G_loss: 0.6933\n",
      "\n",
      "iteration: 46000\n",
      "D_loss: 1.38\n",
      "G_loss: 0.6936\n",
      "\n",
      "iteration: 47000\n",
      "D_loss: 1.382\n",
      "G_loss: 0.6706\n",
      "\n",
      "iteration: 48000\n",
      "D_loss: 1.374\n",
      "G_loss: 0.6987\n",
      "\n",
      "iteration: 49000\n",
      "D_loss: 1.381\n",
      "G_loss: 0.7049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 128\n",
    "# the dimension of the random samples\n",
    "z_dim = 100\n",
    "result_freq = 1000\n",
    "# plot generators' output every figure_iter step\n",
    "figure_iter = 1000\n",
    "max_iter = 50000\n",
    "size1 = 5\n",
    "size2 = 5\n",
    "i = 0\n",
    "\n",
    "total_batch = int((train_num_examples+batch_size-1) / batch_size)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    \n",
    "    if iter % figure_iter == 0:\n",
    "        \n",
    "        # G_sample is a sample from the generator\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_z(size1*size2, z_dim)})\n",
    "\n",
    "        fig1 = plot_sample(samples, size1, size2)\n",
    "        plt.savefig(output_dir + 'GANs' + str(i) + '.png', bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig1)\n",
    "\n",
    "    j = iter%total_batch\n",
    "    start = j * batch_size\n",
    "    end = min(train_num_examples, start + batch_size)\n",
    "    batch_xs = x_train[start:end]\n",
    "    batch_xs = batch_xs.reshape(-1,3072)\n",
    "\n",
    "    _, discriminator_loss = sess.run([D_solver, D_loss], feed_dict={X: batch_xs, Z: sample_z(batch_size, z_dim)})\n",
    "    _, generator_loss     = sess.run([G_solver, G_loss], feed_dict={Z: sample_z(batch_size, z_dim)})\n",
    "\n",
    "    if iter % result_freq == 0:\n",
    "        print('iteration: {}'.format(iter))\n",
    "        print('D_loss: {:0.4}'.format(discriminator_loss))\n",
    "        print('G_loss: {:0.4}'.format(generator_loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
